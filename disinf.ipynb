{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d70750ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:04.325007Z",
     "start_time": "2023-04-11T07:44:04.319811Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import warnings\n",
    "import pandas\n",
    "from string import punctuation, digits\n",
    "import numpy as np\n",
    "from lambeq import BobcatParser, Rewriter\n",
    "from lambeq import remove_cups\n",
    "from lambeq import AtomicType, IQPAnsatz\n",
    "from itertools import chain\n",
    "\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "from lambeq import TketModel\n",
    "from lambeq import BinaryCrossEntropyLoss\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from lambeq import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd22022e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:04.334650Z",
     "start_time": "2023-04-11T07:44:04.327013Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged])\n",
    "    return lemmatized_text\n",
    "\n",
    "def single_content2data(sen):\n",
    "    text=lemmatize_text(sen)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentences_0 = text\n",
    "    sentences_1 = re.sub('<[^<]+?>', '', sentences_0)\n",
    "    table = str.maketrans('', '', punctuation + digits)\n",
    "    sentences_2 = sentences_1.translate(table)\n",
    "    word_tokens = word_tokenize(sentences_2)\n",
    "    filtered_sentence=[]\n",
    "    for w in word_tokens:\n",
    "        if not w.lower() in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = int(line[0])\n",
    "            labels.append([t, 1-t])\n",
    "            sen=line[1:].strip()\n",
    "            sen=lemmatize_text(sen)\n",
    "            sentences.append(sen)\n",
    "    return labels, sentences\n",
    "\n",
    "def diagram_norm(diagram):\n",
    "    rewriter = Rewriter(['prepositional_phrase', 'determiner'])\n",
    "    rewritten_diagram = rewriter(diagram)\n",
    "    normalised_diagram = rewritten_diagram.normal_form()\n",
    "    curry_functor = Rewriter(['curry'])\n",
    "    curried_diagram = curry_functor(normalised_diagram)\n",
    "    return curried_diagram.normal_form()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cd02ccc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:04.388661Z",
     "start_time": "2023-04-11T07:44:04.335658Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels, train_data = read_data('train.txt')\n",
    "dev_labels, dev_data = read_data('dev.txt')\n",
    "test_labels, test_data = read_data('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfd63e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:12.639674Z",
     "start_time": "2023-04-11T07:44:04.389667Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = BobcatParser(verbose='progress')\n",
    "\n",
    "raw_train_diagrams = parser.sentences2diagrams(train_data)\n",
    "raw_dev_diagrams = parser.sentences2diagrams(dev_data)\n",
    "raw_test_diagrams = parser.sentences2diagrams(test_data)\n",
    "# raw_test_diagrams = parser.sentences2diagrams(test_data,tokenised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b6d3d26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:13.147537Z",
     "start_time": "2023-04-11T07:44:12.640679Z"
    }
   },
   "outputs": [],
   "source": [
    "train_diagrams = [remove_cups(diagram) for diagram in raw_train_diagrams]\n",
    "dev_diagrams = [remove_cups(diagram) for diagram in raw_dev_diagrams]\n",
    "test_diagrams = [remove_cups(diagram) for diagram in raw_test_diagrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad469f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:13.182173Z",
     "start_time": "2023-04-11T07:44:13.148542Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_diagrams[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa961b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:15.164139Z",
     "start_time": "2023-04-11T07:44:13.183181Z"
    }
   },
   "outputs": [],
   "source": [
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "P = AtomicType.PREPOSITIONAL_PHRASE\n",
    "NP=AtomicType.NOUN_PHRASE\n",
    "C=AtomicType.CONJUNCTION\n",
    "PU=AtomicType.PUNCTUATION\n",
    "\n",
    "ansatz = IQPAnsatz({N: 1, S: 1, P: 1,NP:1,C:1,PU:1},\n",
    "                   n_layers=1, n_single_qubit_params=2)\n",
    "# n_layers : int\n",
    "#             The number of layers used by the ansatz.\n",
    "#         n_single_qubit_params : int, default: 3\n",
    "#             The number of single qubit rotations used by the ansatz.\n",
    "\n",
    "train_circuits = [ansatz(diagram) for diagram in train_diagrams]\n",
    "dev_circuits =  [ansatz(diagram) for diagram in dev_diagrams]\n",
    "test_circuits = [ansatz(diagram) for diagram in test_diagrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56a3a54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:15.249874Z",
     "start_time": "2023-04-11T07:44:15.165144Z"
    }
   },
   "outputs": [],
   "source": [
    "train_circuits[0].draw(figsize=(4, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76fddbb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:44:15.257270Z",
     "start_time": "2023-04-11T07:44:15.250879Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 2\n",
    "\n",
    "all_circuits = train_circuits+dev_circuits+test_circuits\n",
    "\n",
    "backend = AerBackend()\n",
    "backend_config = {\n",
    "    'backend': backend,\n",
    "    'compilation': backend.default_compilation_pass(2),\n",
    "    'shots': 8192\n",
    "}\n",
    "model = TketModel.from_diagrams(all_circuits, backend_config=backend_config)\n",
    "bce = BinaryCrossEntropyLoss()\n",
    "acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b79f7f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:48:06.627072Z",
     "start_time": "2023-04-11T07:48:06.623334Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=bce,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.05, 'c': 0.06, 'A':0.01*EPOCHS},\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96d44efd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T07:48:13.698872Z",
     "start_time": "2023-04-11T07:48:07.289365Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "\n",
    "train_dataset = Dataset(train_circuits,train_labels,batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset = Dataset(dev_circuits, dev_labels, shuffle=False)\n",
    "\n",
    "trainer.fit(train_dataset, val_dataset, logging_step=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54456396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
